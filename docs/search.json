[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zuzanna Wasiluk",
    "section": "",
    "text": "Education\nBachelor of Arts in Economics Statistics and Data Science concentration Expected Graduation Date May 2025 | Saint Olaf College\n\n\nCurrent Courses\nEconometrics: Time Series and Forecasting Data Science 2 Statistics 2 Politics and Development\n\n\nMembership Organizations\nGirls Write Now Rewriting the Code Women in Economics at St. Olaf College International Association of Feminist Economics"
  },
  {
    "objectID": "zw_mini_project_1a.html",
    "href": "zw_mini_project_1a.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "I produced four choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state. The numerical data was on obesity data and the categorical data was created from a dataset on the number of bikeshare systems. This data is displayed in both static and interactive forms.\n\n# Loading packages \n\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(maps)\nlibrary(mdsr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Loading and renaming my datasets\n\nlibrary(readr)\nnutrition_physical_data &lt;- read_csv(\"Nutrition__Physical_Activity__and_Obesity_-_Behavioral_Risk_Factor_Surveillance_System (2).csv\")\n\nbikeshare &lt;- read_csv(\"Bikeshare_Scooter_Systems_20240919.csv\") |&gt;\n  rename(state = STATE,\n         city = CITY,\n         citystate = CITYSTATE) |&gt;\n  select(state, city, citystate)\n\n\n# New saved data\nobesity_data &lt;- nutrition_physical_data |&gt;\n  #Renaming variable to make it easier to work with\n  rename(age = `Age(years)`) |&gt;\n  #Filtering for a few relevant variables \n  filter(YearStart == 2020,\n         Question == \"Percent of adults aged 18 years and older who have obesity\", \n         !is.na(Data_Value),\n         age == \"18 - 24\"\n  ) |&gt;\n  #Selecting specific columns\n  select(YearStart,\n         LocationDesc, \n         Question, \n         Data_Value, \n         age,\n         LocationAbbr\n  ) |&gt;\n  #Creating a new column with state names that are lowercase \n  mutate(state = str_to_lower(LocationDesc)) |&gt;\n  select(!LocationDesc) |&gt;\n  #Filter out \n  filter(!state == \"guam\" | !state == \"puerto rico\")\n\n#Bring this dataset in so that it can draw points to create states\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nobesity_data |&gt;\n  #Join two datasets with different names for state names (one draws map and the other has numeric variable)\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  #Plotting to create states \n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)\n  ) + \n  #Coloring certain states based on the values in Data_Value\n  geom_polygon(aes(fill = Data_Value), color = \"black\") +\n  scale_fill_viridis(option = \"magma\") +\n  labs(\n    title = \"Percent of Adults Aged 18 years and Older with Obesity,\n    State-Level\",\n    subtitle = \"Data obtained from the U.S. Department of Health & Human Services' \n    Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System\", \n    fill = \"Percentage of Population\",\n    caption = \"Created by Zuzanna Wasiluk for Mini-Project 1\"\n    ) +\n  #Clean white background in the back of the graph\n  coord_map() + \n  theme_void() \n\n\n\n\n\n\n\n\nDescription: This plot describes the percentage of adults who are obese by state as opposed to total population. Generally, most of the states were either magenta or dark purple indicating that they fall between 15-20. Therefore, their percentage of the adult population who have obesity was on the lower half of the scale. There is a concerning pattern of orange and yellow shades in the South region, representing a higher percentage of obese adults in this area than in the rest of the United States. Furthermore, the lightest states are all next to each other which prompts more elaborate investigation into why AR, MS, and AL have such an alarming trend of adult obesity.\nThe link to the website where I found this data can be found here:U.S. Department of Health & Human Services.\nAlt_Text: This is a choropleth plot of the United States (48 states minus Hawaii and Alaska) that visualizes percentages of adult obesity in each state. It has a continuous scale alongside the map graph with a range of 11 to 27, the scale is labeled as “Percentage of Population.” The variables are state and percentage of adult population with obesity. The appearance of the colors for most of the states suggests that the total adult population who have obesity floats around 18 percent (the middle of the scale) for the United States. However, the light yellow and orange color in the Southern region tells us that the percentage of adult obesity is higher in the South.\n\n#Create categorical variable\nbikeshare_state &lt;- bikeshare |&gt;\n  #Group by state to find the number of cities in each state with a bikeshare scooter system\n  group_by(state) |&gt;\n  summarise(Count = n()) |&gt;\n  #Mutate to create a column that takes numeric information and groups it based on which conditions it ultimately meets\n  mutate(Amount = case_when(\n    Count &gt;= 130 ~ \"Very High\",\n    Count &gt;= 100 ~ \"High\",\n    Count &gt;= 80 ~ \"Moderate\",\n    Count &gt;= 40 ~ \"Low Moderate\",\n    Count &gt;= 0 ~ \"Low\"))\n\n\n#Group by to have a dataset to merge with the other one that contains the written out state name instead of abbreviation \nnutrition &lt;- nutrition_physical_data |&gt;\n  group_by(LocationDesc, LocationAbbr) |&gt;\n  summarize(n = n())\n\n\n#Merge the dataset to have bstates with the lowercase versions of the state names \nbstates &lt;- bikeshare_state |&gt;\n  right_join(nutrition, by = c(\"state\" = \"LocationAbbr\")) |&gt;\n  mutate(region = str_to_lower(LocationDesc)) |&gt;\n  select(!n)\n\n\n#Join with the dataset that contains information for drawing the states in the plot using the geometry column\nbstates |&gt;\n  right_join(us_states, by = \"region\")|&gt;\n  #Relevel variables so that they appear in the appropriate order on the plot\n  mutate(Amount = fct_relevel(Amount, \"Very High\", \"High\", \"Moderate\", \"Low Moderate\", \"Low\")) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  #Shade the state shapes by their value in the Amount variable \n  geom_polygon(aes(fill = Amount), color = \"black\", linewidth = 0.2) +\n  #Clean background\n  coord_map() + \n  theme_void() +  \n  scale_fill_viridis_d(option = \"magma\") + \n  labs(\n    title = \"Classification of Bikeshare Scooter Systems in Each \n    State in the United States\",\n    subtitle = \"Data obtained from the U.S. Department of Transportation\n    from their Bureau of Transportation Statistics (BTS)\",\n    fill = \"Classification\",\n    caption = \"Created by Zuzanna Wasiluk for Mini-Project 1\"\n  )\n\n\n\n\n\n\n\n\nThe plot above describes the “classification” of bikeshare scooter systems in US states. To provide context for those less familiar with bikeshare scooter systems, they are a category of transportation that includes small vehicles that are typically found in cities. We find that most US states have a low number of bikeshare scooter systems except states with multiple major US cities (Florida, California, Texas). From the plot before, we see that the states with the highest obesity percentage also have the lowest classification of bikeshare scooter systems. However, this lack does not provide an explanation for the obesity rates because many US states with lower obesity rates also have a low number of bikeshare scooter systems.\nThe data can be found under this link Bureau of Transportation Statistics’ website.\n\nlibrary(leaflet)\nlibrary(sf)\nlibrary(htmltools)\nlibrary(glue)\n\n\n# Create density bins \nweight_int_state &lt;- obesity_data |&gt;\n  mutate(density = cut(Data_Value, n = 6,\n          breaks = c(0, 5, 10, 15, 20, 25, 30))) |&gt;\n  filter(!(state %in% c(\"alaska\", \"hawaii\", \"puerto rico\", \"national\")))\n\n#Create labels by combining multiple strings into one string \nweight_int_state &lt;- weight_int_state |&gt;\n  mutate(labels = str_c(state, \": \", Data_Value, \" % of adult population\"))\n\nlabels &lt;- lapply(weight_int_state$labels, HTML)\n\n#Create a new saved dataset\nleaflet_data &lt;- weight_int_state |&gt;\n  select(density, state, labels, Data_Value)\n\n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")  \nclass(states) \n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n#Create a new variable with lowercase letters\nn_states &lt;- states |&gt;\n  mutate(lowname = str_to_lower(name))\n\nnb_states &lt;- n_states |&gt;\n  left_join(leaflet_data, by = c(\"lowname\" = \"state\"))\n#Define bins and palette for leaflet operations\nbins &lt;- c(0, 5, 10, 15, 20, 25, 30)\npal &lt;- colorBin(\"YlOrRd\", domain = nb_states$Data_Value, bins = bins)\n\n\n#Leaflet plot\nleaflet(nb_states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Data_Value),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) %&gt;%\n  addLegend(pal = pal, values = ~Data_Value, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data\n\n\n\n\n\n\nCategorical interactive plot\n\ncat_states &lt;- nb_states |&gt;\n  left_join(bstates, by =c(\"lowname\" = \"region\")) |&gt;\n  #Putting multiple strings in one string using str_c\nmutate(labels = str_c(state, \": \", Amount, \" number of bikeshare systems\")) |&gt;\n  mutate(Amount = as.factor(Amount))\n\nlabels &lt;- lapply(cat_states$labels, HTML)\n#Define levels and factor palette\nlevels(cat_states$Amount)\n\n[1] \"High\"         \"Low\"          \"Low Moderate\" \"Moderate\"     \"Very High\"   \n\nfactpal &lt;- colorFactor(\"viridis\",\n                       levels(cat_states$Amount))\n#Leaflet plot\nleaflet(cat_states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 2,\n    opacity = 1,\n    color = \"black\",\n    fillColor = ~ factpal(cat_states$Amount),\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = factpal, values = ~cat_states$Amount, \n            opacity = 0.7, title = NULL, position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data"
  },
  {
    "objectID": "project_file_gz.html",
    "href": "project_file_gz.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "project_file_gz.html#penguin-random-house-web-scraping",
    "href": "project_file_gz.html#penguin-random-house-web-scraping",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "project_file_gz.html#ethical-considerations",
    "href": "project_file_gz.html#ethical-considerations",
    "title": "Mini Project 2",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nWe opted to use the robot.txt paths to determine if data from Penguin’s book events was permitted for web scraping. The site’s robots.txt file allowed our bots to access and scrape the data. While we considered implementing a polite function to ensure a respectful approach to data retrieval, this step appeared unnecessary given that the data is public and intended for widespread use."
  },
  {
    "objectID": "project_file_gz.html#novel-insights-potential-and-justification",
    "href": "project_file_gz.html#novel-insights-potential-and-justification",
    "title": "Mini Project 2",
    "section": "Novel Insights Potential and Justification",
    "text": "Novel Insights Potential and Justification\nOur final tibble will hold important information for booksellers, authors, agents, and students to utilize in regards to books/authors from Penguin Random House. We were initially motivated to explore book events from Penguin Random House to inform student decisions to network with agents and authors at various events.\nStudents can use our data to answer questions such as:\n\n“Where are events most commonly held?”\n“Which season has the most book events?”\n“What are the best events to attend to network with the right authors and book genres?”\n\nUpon further reflection, we discovered that our data could also be used by booksellers, book agents, and authors. Booksellers and authors may find our data useful because they can analyze current trends with where authors are going (chain or independent bookstore) and what authors are successful in book events (if we assume multiple book events equals a marketable author).Book agents within Penguin Random House or outside of it (smaller boutique literary agencies or other Big Five publishers) can use our data to answer questions on which authors are holding events, when a certain book is no longer welcomed in event spaces, and perhaps even publicity tactics. This data has relevant applications for different data needs within the publishing industry and for creating engaging data visualizations (including static or leaflet maps).\n\n#Step 0: Check if the website allows scraping \nrobotstxt::paths_allowed(\"https://www.penguinrandomhouse.com/authors/events/\")\n\n[1] TRUE\n\n#Extract individual information from the events page \ninfo_from_page &lt;- function(event, css_selector) {\n  read_html(event) |&gt; \n#Extracting nodes from the XML by using the CSS path from selector\n  html_nodes(css_selector) |&gt; \n#Extracting text\n  html_text()\n}\n\n#Test, the function works\ninfo_from_page(\"https://www.penguinrandomhouse.com/authors/events/\", \".date-display\")\n\n[1] \"November 2024\"\n\n\n\n#Scrape info using the CSS path and compile it into a tibble \nscrape_events &lt;- function(url){\n  \n  date &lt;- info_from_page(url, \".start\")\n  book &lt;- info_from_page(url, \".author-of a\")\n  author &lt;- info_from_page(url, \".author-name:nth-child(1)\")\n  host &lt;- info_from_page(url, \".event-location .hdr\")\n  state &lt;- info_from_page(url, \"span:nth-child(4)\")\n  zip_code &lt;- info_from_page(url, \"span:nth-child(5)\")\n  \n  tibble(date = date, \n           book = book, \n           author = author,\n           host = host,\n           state = state)\n  \n}\n\n\n#Test to see that our tibble looks appropriate\nscrape_events(\"https://www.penguinrandomhouse.com/authors/events/?page=2\")\n\n# A tibble: 10 × 5\n   date                 book                                  author host  state\n   &lt;chr&gt;                &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 at 10:45am All the Colors of the Dark: A Read w… Chris… Litc… SC   \n 2 11/4/2024 at 6:00 PM The Swifts: A Gallery of Rogues       Beth … QUAI… NC   \n 3 11/4/2024 at 7pm     Every Valley                          Charl… Poli… DC   \n 4 11/4/2024 at 6:00    Blackness Is a Gift I Can Give Her    R. Re… ANOT… ON   \n 5 11/4/2024            Classic German Cooking                Luisa… Vass… NY   \n 6 11/5/2024            What She Said                         Eliza… Arts… ON –…\n 7 11/5/2024 at 11AM    The Gulf                              Adam … TINL… ON   \n 8 11/5/2024 at 7pm     Laugh More                            Debbi… Cent… AB   \n 9 11/6/2024 at 12:00pm This Fierce People                    Alan … Geor… VA   \n10 11/6/2024            Fragments of a Paradise               Paul … TYPE… ON   \n\n\n\n#This for loop runs all of the months and all of the days\n#   in one chunk but it is not the most efficient \n\n#If someone is interested in keeping this method in one chunk\n#   they can use this code for the nested for loop.\n\n#Nested for loop with i for months and j for days. \nfor(i in c(10, 11, 12, 1, 2, 3, 4)){\n#Runs to find data for all of the dates in these months\n#   we can compile all of the data together \n  for(j in 1:31){\n#Combining i and j for the dates to keep track of event dates\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    scrape_events(url)\n  }\n}\n\n\n#Test chunk to see if our previous code worked with a smaller set of data\n\n#If you wanted to run the previous code chunk, this test \n#   proves that it will give you a larger version\n\ni=11\nj=1\ndate = str_c(i, \"/\", j, \"/\", \"2024\")\nurl = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\nscrape_events(url)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: date &lt;chr&gt;, book &lt;chr&gt;, author &lt;chr&gt;, host &lt;chr&gt;, state &lt;chr&gt;\n\n\n\n#Running each individual month as a separate \n#   for loop to be more efficient\n\n#Create a list to store your scraped data\noctober &lt;- list()\n  i=10\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    october[[j]] &lt;- scrape_events(url)\n}\n\n#Create a tibble from the list \noctober_tibble &lt;- bind_rows(october) |&gt; \n  as_tibble()\n  \nnovember &lt;- list()\n  i=11\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    november[[j]] &lt;- scrape_events(url)\n}\n  \n  november_tibble &lt;- bind_rows(november) |&gt; \n    as_tibble() \n  \ndecember &lt;- list()\n  i=12\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    december[[j]] &lt;- scrape_events(url)\n}\n  \n  december_tibble &lt;- bind_rows(december) |&gt; \n    as_tibble()\n  \njanuary &lt;- list()\n  i=1\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    january[[j]] &lt;- scrape_events(url)\n}\n  \njanuary_tibble &lt;- bind_rows(january) |&gt; \n  as_tibble()\n\nfebruary &lt;- list()\n  i=2\nfor(j in 1:28){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    february[[j]] &lt;- scrape_events(url)\n}\n  \n  february_tibble &lt;- bind_rows(february) |&gt; \n    as_tibble() \n  \nmarch &lt;- list()\n  i=3\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    march[[j]] &lt;- scrape_events(url)\n}\n  \n  march_tibble &lt;- bind_rows(march) |&gt; \n    as_tibble() \n  \napril &lt;- list()\n  i=4\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    april[[j]] &lt;- scrape_events(url)\n}\n  \napril_tibble &lt;- bind_rows(april) |&gt; \n  as_tibble()\n\n\n#Bind all of the tibbles from the previous chunk \n#   together to create one big tibble called events \n\nevents &lt;- rbind(october_tibble,\n                 november_tibble,\n                 december_tibble,\n                 january_tibble,\n                 february_tibble,\n                 march_tibble,\n                 april_tibble)\nevents\n\n# A tibble: 591 × 5\n   date                 book                                  author host  state\n   &lt;chr&gt;                &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 at 10:45am All the Colors of the Dark: A Read w… Chris… Litc… SC   \n 2 11/4/2024 at 6:00 PM The Swifts: A Gallery of Rogues       Beth … QUAI… NC   \n 3 11/4/2024 at 7pm     Every Valley                          Charl… Poli… DC   \n 4 11/4/2024 at 6:00    Blackness Is a Gift I Can Give Her    R. Re… ANOT… ON   \n 5 11/4/2024            Classic German Cooking                Luisa… Vass… NY   \n 6 11/5/2024            What She Said                         Eliza… Arts… ON –…\n 7 11/5/2024 at 11AM    The Gulf                              Adam … TINL… ON   \n 8 11/5/2024 at 7pm     Laugh More                            Debbi… Cent… AB   \n 9 11/6/2024 at 12:00pm This Fierce People                    Alan … Geor… VA   \n10 11/6/2024            Fragments of a Paradise               Jean … TYPE… ON   \n# ℹ 581 more rows\n\n\n\npenguin_events &lt;- events |&gt; \n#Separate the time from date to create a separate column\n#   for the time of the events\n  separate(date, into = c(\"date\", \"time\"), sep = \" at \") |&gt; \n#Some of the host names were all caps or had other \n#   abnormalities that we needed to fix\n  mutate(host = str_to_title(host),\n#Some observations had strange patterns in the text (r/n) \n#   that distracted from the host's name \n         host = str_replace_all(host, \"[\\r\\n]\", \" \"))\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 191 rows [5, 6, 10, 14,\n15, 17, 21, 23, 24, 31, 35, 40, 48, 49, 51, 56, 67, 68, 74, 79, ...].\n\npenguin_events\n\n# A tibble: 591 × 6\n   date      time    book                                     author host  state\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                                    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 10:45am All the Colors of the Dark: A Read with… Chris… Litc… SC   \n 2 11/4/2024 6:00 PM The Swifts: A Gallery of Rogues          Beth … Quai… NC   \n 3 11/4/2024 7pm     Every Valley                             Charl… Poli… DC   \n 4 11/4/2024 6:00    Blackness Is a Gift I Can Give Her       R. Re… Anot… ON   \n 5 11/4/2024 &lt;NA&gt;    Classic German Cooking                   Luisa… Vass… NY   \n 6 11/5/2024 &lt;NA&gt;    What She Said                            Eliza… Arts… ON –…\n 7 11/5/2024 11AM    The Gulf                                 Adam … Tinl… ON   \n 8 11/5/2024 7pm     Laugh More                               Debbi… Cent… AB   \n 9 11/6/2024 12:00pm This Fierce People                       Alan … Geor… VA   \n10 11/6/2024 &lt;NA&gt;    Fragments of a Paradise                  Jean … Type… ON   \n# ℹ 581 more rows"
  },
  {
    "objectID": "penguin.html",
    "href": "penguin.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "penguin.html#penguin-random-house-web-scraping",
    "href": "penguin.html#penguin-random-house-web-scraping",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "penguin.html#ethical-considerations",
    "href": "penguin.html#ethical-considerations",
    "title": "Mini Project 2",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nWe opted to use the robot.txt paths to determine if data from Penguin’s book events was permitted for web scraping. The site’s robots.txt file allowed our bots to access and scrape the data. While we considered implementing a polite function to ensure a respectful approach to data retrieval, this step appeared unnecessary given that the data is public and intended for widespread use."
  },
  {
    "objectID": "penguin.html#novel-insights-potential-and-justification",
    "href": "penguin.html#novel-insights-potential-and-justification",
    "title": "Mini Project 2",
    "section": "Novel Insights Potential and Justification",
    "text": "Novel Insights Potential and Justification\nOur final tibble will hold important information for booksellers, authors, agents, and students to utilize in regards to books/authors from Penguin Random House. We were initially motivated to explore book events from Penguin Random House to inform student decisions to network with agents and authors at various events.\nStudents can use our data to answer questions such as:\n\n“Where are events most commonly held?”\n“Which season has the most book events?”\n“What are the best events to attend to network with the right authors and book genres?”\n\nUpon further reflection, we discovered that our data could also be used by booksellers, book agents, and authors. Booksellers and authors may find our data useful because they can analyze current trends with where authors are going (chain or independent bookstore) and what authors are successful in book events (if we assume multiple book events equals a marketable author).Book agents within Penguin Random House or outside of it (smaller boutique literary agencies or other Big Five publishers) can use our data to answer questions on which authors are holding events, when a certain book is no longer welcomed in event spaces, and perhaps even publicity tactics. This data has relevant applications for different data needs within the publishing industry and for creating engaging data visualizations (including static or leaflet maps).\n\n#Step 0: Check if the website allows scraping \nrobotstxt::paths_allowed(\"https://www.penguinrandomhouse.com/authors/events/\")\n\n[1] TRUE\n\n#Extract individual information from the events page \ninfo_from_page &lt;- function(event, css_selector) {\n  read_html(event) |&gt; \n#Extracting nodes from the XML by using the CSS path from selector\n  html_nodes(css_selector) |&gt; \n#Extracting text\n  html_text()\n}\n\n#Test, the function works\ninfo_from_page(\"https://www.penguinrandomhouse.com/authors/events/\", \".date-display\")\n\n[1] \"November 2024\"\n\n\n\n#Scrape info using the CSS path and compile it into a tibble \nscrape_events &lt;- function(url){\n  \n  date &lt;- info_from_page(url, \".start\")\n  book &lt;- info_from_page(url, \".author-of a\")\n  author &lt;- info_from_page(url, \".author-name:nth-child(1)\")\n  host &lt;- info_from_page(url, \".event-location .hdr\")\n  state &lt;- info_from_page(url, \"span:nth-child(4)\")\n  zip_code &lt;- info_from_page(url, \"span:nth-child(5)\")\n  \n  tibble(date = date, \n           book = book, \n           author = author,\n           host = host,\n           state = state)\n  \n}\n\n\n#Test to see that our tibble looks appropriate\nscrape_events(\"https://www.penguinrandomhouse.com/authors/events/?page=2\")\n\n# A tibble: 10 × 5\n   date                 book                                  author host  state\n   &lt;chr&gt;                &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 at 10:45am All the Colors of the Dark: A Read w… Chris… Litc… SC   \n 2 11/4/2024 at 6:00 PM The Swifts: A Gallery of Rogues       Beth … QUAI… NC   \n 3 11/4/2024 at 7pm     Every Valley                          Charl… Poli… DC   \n 4 11/4/2024 at 6:00    Blackness Is a Gift I Can Give Her    R. Re… ANOT… ON   \n 5 11/4/2024            Classic German Cooking                Luisa… Vass… NY   \n 6 11/5/2024            What She Said                         Eliza… Arts… ON –…\n 7 11/5/2024 at 11AM    The Gulf                              Adam … TINL… ON   \n 8 11/5/2024 at 7pm     Laugh More                            Debbi… Cent… AB   \n 9 11/6/2024 at 12:00pm This Fierce People                    Alan … Geor… VA   \n10 11/6/2024            Fragments of a Paradise               Paul … TYPE… ON   \n\n\n\n#This for loop runs all of the months and all of the days\n#   in one chunk but it is not the most efficient \n\n#If someone is interested in keeping this method in one chunk\n#   they can use this code for the nested for loop.\n\n#Nested for loop with i for months and j for days. \nfor(i in c(10, 11, 12, 1, 2, 3, 4)){\n#Runs to find data for all of the dates in these months\n#   we can compile all of the data together \n  for(j in 1:31){\n#Combining i and j for the dates to keep track of event dates\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    scrape_events(url)\n  }\n}\n\n\n#Test chunk to see if our previous code worked with a smaller set of data\n\n#If you wanted to run the previous code chunk, this test \n#   proves that it will give you a larger version\n\ni=11\nj=1\ndate = str_c(i, \"/\", j, \"/\", \"2024\")\nurl = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\nscrape_events(url)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: date &lt;chr&gt;, book &lt;chr&gt;, author &lt;chr&gt;, host &lt;chr&gt;, state &lt;chr&gt;\n\n\n\n#Running each individual month as a separate \n#   for loop to be more efficient\n\n#Create a list to store your scraped data\noctober &lt;- list()\n  i=10\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    october[[j]] &lt;- scrape_events(url)\n}\n\n#Create a tibble from the list \noctober_tibble &lt;- bind_rows(october) |&gt; \n  as_tibble()\n  \nnovember &lt;- list()\n  i=11\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    november[[j]] &lt;- scrape_events(url)\n}\n  \n  november_tibble &lt;- bind_rows(november) |&gt; \n    as_tibble() \n  \ndecember &lt;- list()\n  i=12\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    december[[j]] &lt;- scrape_events(url)\n}\n  \n  december_tibble &lt;- bind_rows(december) |&gt; \n    as_tibble()\n  \njanuary &lt;- list()\n  i=1\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    january[[j]] &lt;- scrape_events(url)\n}\n  \njanuary_tibble &lt;- bind_rows(january) |&gt; \n  as_tibble()\n\nfebruary &lt;- list()\n  i=2\nfor(j in 1:28){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    february[[j]] &lt;- scrape_events(url)\n}\n  \n  february_tibble &lt;- bind_rows(february) |&gt; \n    as_tibble() \n  \nmarch &lt;- list()\n  i=3\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    march[[j]] &lt;- scrape_events(url)\n}\n  \n  march_tibble &lt;- bind_rows(march) |&gt; \n    as_tibble() \n  \napril &lt;- list()\n  i=4\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    april[[j]] &lt;- scrape_events(url)\n}\n  \napril_tibble &lt;- bind_rows(april) |&gt; \n  as_tibble()\n\n\n#Bind all of the tibbles from the previous chunk \n#   together to create one big tibble called events \n\nevents &lt;- rbind(october_tibble,\n                 november_tibble,\n                 december_tibble,\n                 january_tibble,\n                 february_tibble,\n                 march_tibble,\n                 april_tibble)\nevents\n\n# A tibble: 591 × 5\n   date                 book                                  author host  state\n   &lt;chr&gt;                &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 at 10:45am All the Colors of the Dark: A Read w… Chris… Litc… SC   \n 2 11/4/2024 at 6:00 PM The Swifts: A Gallery of Rogues       Beth … QUAI… NC   \n 3 11/4/2024 at 7pm     Every Valley                          Charl… Poli… DC   \n 4 11/4/2024 at 6:00    Blackness Is a Gift I Can Give Her    R. Re… ANOT… ON   \n 5 11/4/2024            Classic German Cooking                Luisa… Vass… NY   \n 6 11/5/2024            What She Said                         Eliza… Arts… ON –…\n 7 11/5/2024 at 11AM    The Gulf                              Adam … TINL… ON   \n 8 11/5/2024 at 7pm     Laugh More                            Debbi… Cent… AB   \n 9 11/6/2024 at 12:00pm This Fierce People                    Alan … Geor… VA   \n10 11/6/2024            Fragments of a Paradise               Jean … TYPE… ON   \n# ℹ 581 more rows\n\n\n\npenguin_events &lt;- events |&gt; \n#Separate the time from date to create a separate column\n#   for the time of the events\n  separate(date, into = c(\"date\", \"time\"), sep = \" at \") |&gt; \n#Some of the host names were all caps or had other \n#   abnormalities that we needed to fix\n  mutate(host = str_to_title(host),\n#Some observations had strange patterns in the text (r/n) \n#   that distracted from the host's name \n         host = str_replace_all(host, \"[\\r\\n]\", \" \"))\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 191 rows [5, 6, 10, 14,\n15, 17, 21, 23, 24, 31, 35, 40, 48, 49, 51, 56, 67, 68, 74, 79, ...].\n\npenguin_events\n\n# A tibble: 591 × 6\n   date      time    book                                     author host  state\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                                    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 11/4/2024 10:45am All the Colors of the Dark: A Read with… Chris… Litc… SC   \n 2 11/4/2024 6:00 PM The Swifts: A Gallery of Rogues          Beth … Quai… NC   \n 3 11/4/2024 7pm     Every Valley                             Charl… Poli… DC   \n 4 11/4/2024 6:00    Blackness Is a Gift I Can Give Her       R. Re… Anot… ON   \n 5 11/4/2024 &lt;NA&gt;    Classic German Cooking                   Luisa… Vass… NY   \n 6 11/5/2024 &lt;NA&gt;    What She Said                            Eliza… Arts… ON –…\n 7 11/5/2024 11AM    The Gulf                                 Adam … Tinl… ON   \n 8 11/5/2024 7pm     Laugh More                               Debbi… Cent… AB   \n 9 11/6/2024 12:00pm This Fierce People                       Alan … Geor… VA   \n10 11/6/2024 &lt;NA&gt;    Fragments of a Paradise                  Jean … Type… ON   \n# ℹ 581 more rows"
  },
  {
    "objectID": "econ.html",
    "href": "econ.html",
    "title": "Economics",
    "section": "",
    "text": "Submitted: May 21, 2024 Instructor: Professor Colin Harris\nTitle: The Cultural Impact of Joining the European Union on 2004 and 2007 Expansion Countries\nAbstract:\nTwo of the most historically significant expansions in the European Union (EU) occurred in 2004 and 2007 with mostly former post-communist countries and transition economies. These EU enlargements unified the West and East after a period of Eastern isolation from the rest of Europe. Since then, the EU has expanded its influence outside of the purely economic into the social, cultural, and political. With this shift and the goal to curate a national identity for Europe, do member states experience converge to the values of the EU founding members? Using a staggered difference-in-difference model, I compared the average value of 2004/2007 expansion countries to the EU founding member average value before and after treatment (membership into the EU). This study concludes that convergence only occurs for tolerance toward racial minorities and immigrants.\nJEL codes: D91, A13, Z13\nKeywords: social convergence, social divergence, the European Union, 2004 member states, 2007 member states, EU founding members"
  },
  {
    "objectID": "econ.html#download-the-pdf",
    "href": "econ.html#download-the-pdf",
    "title": "Economics",
    "section": "Download the PDF",
    "text": "Download the PDF"
  }
]